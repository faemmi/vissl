# @package _global_
config:
  VERBOSE: False
  LOG_FREQUENCY: 100
  TEST_ONLY: False
  TEST_MODEL: False
  SEED_VALUE: 0
  MULTI_PROCESSING_METHOD: forkserver
  HOOKS:
    PERF_STATS:
      MONITOR_PERF_STATS: True  # monitoring statisics like forward time, backward time, loss time
      ROLLING_BTIME_FREQ: -1   # rolling average batch time
    TENSORBOARD_SETUP:
      USE_TENSORBOARD: True
      LOG_DIR: /p/project/deepacf/maelstrom/emmerich1/deepclusterv2/checkpoints
      EXPERIMENT_LOG_DIR: "tensorboard"
      FLUSH_EVERY_N_MIN: 5
      LOG_PARAMS_GRADIENTS: True
      LOG_PARAMS: True
      LOG_PARAMS_EVERY_N_ITERS: -1
  DATA:
    NUM_DATALOADER_WORKERS: 1   # Set this depending on the number of CPUs you have
    TRAIN:
      DATA_SOURCES: [disk_folder] # can be user specified
      LABEL_SOURCES: [disk_folder] # can be user specified
      DATASET_NAMES: [remote_dataset_jsc_hourly]
      # Number of unique samples in minibatch per GPU
      BATCHSIZE_PER_REPLICA: 64
      # either standard | sample_index | zero
      LABEL_TYPE: sample_index    # just an implementation detail. Label isn't used
      # Somehow the data loader wrapping in data/__init__.py#L218 (DataloaderAsyncGPUWrapper)
      # causes the dataloader to return a tensor of 0s for the labels,
      # which should actually return the sample indexes (`LABEL_TYPE`).
      # Hence, we manually set the target key sames to `data_idx` instead of `labels`.
      TARGET_KEY_NAMES: ["data_idx"]
      TRANSFORMS:                 # https://vissl.readthedocs.io/en/latest/api/data.html
        - name: ImgPilToMultiCrop
          total_num_crops: 2
          size_crops: [96]
          num_crops: [2]
          crop_scales: [[0.08, 1]]
        #- name: ImgPilToMultiCrop
        #  total_num_crops: 6
        #  size_crops: [160, 96]
        #  num_crops: [2, 4]
        #  crop_scales: [[0.08, 1], [0.05, 0.14]]
        #- name: RandomHorizontalFlip # Horizontally flip the given image randomly with a given probability
        #  p: 0.5
        #- name: ImgPilColorDistortion # Apply Random color distortions to the input image. SimCLR - hue, saturation, brightness, grayscale
        #  strength: 1.0
        #- name: ImgPilGaussianBlur # SimCLR Apply Gaussian Blur to the PIL image. Take the radius and probability of application as the parameter.
        #  p: 0.5
        #  radius_min: 0.1
        #  radius_max: 2.0
        - name: ToTensor          # (H x W x C)[0, 255] to (C x H x W) [0.0, 1.0]
        - name: Normalize
          # mean: [0.485, 0.456, 0.406]
          # std: [0.229, 0.224, 0.225]
          # TODO: Why this mean/std?
          mean: [0.011, 0.011, 0.011]
          std: [0.077, 0.077, 0.077]
          #mean: [0.04501121746359247, ]
          #std: [0.09018350753501449, ]
      COLLATE_FUNCTION: multicrop_collator # SwAV - The collators collates the batch assuming k-copies of imput image
      MMAP_MODE: True # whether to memory map the input data
      COPY_TO_LOCAL_DISK: False # whether the data specified (whether file list or directory) should be copied locally on the machine where training is happening.
      COPY_DESTINATION_DIR: /tmp/imagenet1k/ # COPY_TO_LOCAL_DISK is set to FALSE
      DROP_LAST: True
  TRAINER:
    TRAIN_STEP_NAME: standard_train_step
  METERS:
    name: ""
  MODEL:
    # Input type also allows integers, which represents
    # the number of input channels
    INPUT_TYPE: 3
    TRUNK:
      NAME: resnet
      RESNETS:
        DEPTH: 50
    HEAD:
      PARAMS: [
        ["mlp", {"dims": [2048, 2048], "use_relu": True, "skip_last_layer_relu_bn": False}],
        ["mlp", {"dims": [2048, 128]}],
      ]
    TEMP_FROZEN_PARAMS_ITER_MAP: [] # parameters are frozen for specified iterations and then start training
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: True  # convert all the BatchNorm layers to Global BatchNorm
      SYNC_BN_TYPE: apex
      GROUP_SIZE: 8 # if group_size>0 -> will set group_size=value set by user.
    AMP_PARAMS:
      USE_AMP: True  # leverages mixed precision training by default
      AMP_ARGS: {"opt_level": "O1"}  # Use O1 as it is robust and stable than O3
  LOSS:
    name: deepclusterv2_loss
    deepclusterv2_loss:
      num_train_samples: 35064
      #num_train_samples: 54864
      num_crops: 2
      temperature: 0.1
      #num_clusters: [3000, 3000, 3000]
      num_clusters: [30, 30, 30]
      #num_clusters: [7, 7, 7]
      kmeans_iters: 10
      memory_params:
        crops_for_mb: [0, 1]
        # Embedding dim must match output dimension of last MLP layer
        embedding_dim: 128
      output_dir: /tmp/deepclusterv2
  OPTIMIZER:
      name: sgd
      use_larc: True # ratio between gradient and parameter magnitudes is used to calculate an adaptive local learning rate for each individual parameter
      larc_config: # algorithm is designed to improve convergence of large batch training.
        clip: False
        trust_coefficient: 0.001
        eps: 0.00000001
      weight_decay: 0.000001
      momentum: 0.9
      nesterov: False
      # SPECIFY NUMBER OF EPOCHS
      num_epochs: 1
      regularize_bn: True # whether to regularize batch norm. if set to False, weight decay of batch norm params is 0
      regularize_bias: True # whether to regularize bias parameter. if set to False, weight decay of bias params is 0.
      param_schedulers:
        lr:  # convenient to scale Learning rate automatically as per the scaling
          auto_lr_scaling:
          # scaled_lr is calculated: scaled_lr = ((batchsize_per_gpu * world_size) * base_value ) / base_lr_batch_size
            auto_scale: true # if set to True, learning rate will be scaled
            base_value: 0.3 # base learning rate value that will be scaled.
            base_lr_batch_size: 256 # batch size for which the base learning rate is specified
          name: composite    # Linear warmup + cosine with restarts
          schedulers:
            - name: linear
              start_value: 0.3
              end_value: 4.8
            - name: cosine
              start_value: 4.8
              end_value: 0.0000
          update_interval: step
          interval_scaling: [rescaled, fixed]
          # lengths: [0.1, 0.9]                 # 100ep FOR EPOCHS VARY .>>
          # lengths: [0.05, 0.95]             # 200ep
          # lengths: [0.025, 0.975]           # 400ep
          # lengths: [0.02, 0.98]             # 500ep
          # lengths: [0.0166667, 0.9833333]   # 600ep
          # lengths: [0.0125, 0.9875]         # 800ep
          # lengths: [0.01, 0.99]             # 1000ep
          lengths: [0.0128, 0.9872]         # 1ep IG-1B
          # lengths: [0.00641, 0.99359]       # 2ep IG-1B
          # lengths: [0.002563, 0.997437]     # 5ep IG-1B = 50 ep IG-100M
  MACHINE:
    DEVICE: gpu
  SLURM:
    USE_SLURM: false
  DISTRIBUTED:
    BACKEND: nccl
    NUM_NODES: 1    # number of machines to use in training. Each machine can have many gpus. NODES count number of unique hosts
    NUM_PROC_PER_NODE: 4 # set this to the number of gpus per machine.
    RUN_ID: auto  #"localhost:49160" #  #(date +'%Y%m%d')/specify run_id={master_node}:{port}/unique rendezvous run_id that is specific to 1 run.
    INIT_METHOD: tcp  # this could be tcp | env | file or any other pytorch supported methods
    # every training run should have a unique id. Following are the options:
    #   1. If using INIT_METHOD=env, RUN_ID="" is fine.
    #   2. If using INIT_METHOD=tcp,
    #      - if you use > 1 machine, set port yourself. RUN_ID="localhost:{port}".
    #      - If using 1 machine, set RUN_ID=auto and a free port will be automatically selected
    #   3. IF using INIT_METHOD=file, RUN_ID={file_path}
    NCCL_DEBUG: True
  CHECKPOINT:
    AUTO_RESUME: True
    DIR: /p/project/deepacf/maelstrom/emmerich1/deepclusterv2/checkpoints
    CHECKPOINT_FREQUENCY: 25           # every phase since data is big
    CHECKPOINT_ITER_FREQUENCY: -1    # checkpoint model at various iterations as well and not just phase
    OVERWRITE_EXISTING: true
